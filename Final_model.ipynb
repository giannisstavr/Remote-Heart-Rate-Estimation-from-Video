{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vJJ3W9Jdc4N",
        "outputId": "f4e76c74-3728-4105-a2bc-c29c76fdaf2a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers, models, callbacks\n",
        "#from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "from keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# 1. Set Seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# 2. Define Directories & Params\n",
        "DATA_DIR = '/home/giannisstavrakis/Documents/Pure processed full'  \n",
        "SEQUENCE_LENGTH = 30\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "# 3. Split the dataset\n",
        "all_folders = sorted([\n",
        "    folder for folder in os.listdir(DATA_DIR)\n",
        "    if os.path.isdir(os.path.join(DATA_DIR, folder))\n",
        "])\n",
        "print(f\"Total folders found: {len(all_folders)}\")\n",
        "\n",
        "# Train/Val/Test split\n",
        "train_folders, temp_folders = train_test_split(\n",
        "    all_folders, test_size=0.3, random_state=SEED, shuffle=True\n",
        ")\n",
        "val_folders, test_folders = train_test_split(\n",
        "    temp_folders, test_size=0.5, random_state=SEED, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"Training folders: {len(train_folders)}\")\n",
        "print(f\"Validation folders: {len(val_folders)}\")\n",
        "print(f\"Test folders: {len(test_folders)}\")\n",
        "\n",
        "# 4. Utility function to build (seq_image_paths, label) pairs\n",
        "def get_image_label_pairs(folder_list, sequence_length, data_dir):\n",
        "    \"\"\"\n",
        "    Returns a list of (list_of_image_paths, label).\n",
        "    Each `list_of_image_paths` has exactly SEQUENCE_LENGTH file paths,\n",
        "    and `label` is the pulse rate of the last frame in the sequence.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    for folder_name in folder_list:\n",
        "        folder_path = os.path.join(data_dir, folder_name)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        json_path = os.path.join(folder_path, 'aligned_pulse_rate.json')\n",
        "        if not os.path.exists(json_path):\n",
        "            continue\n",
        "\n",
        "        # Load JSON data\n",
        "        with open(json_path, 'r') as f:\n",
        "            try:\n",
        "                pulse_data = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "        # Filter and sort\n",
        "        valid_data = [d for d in pulse_data if 'ImageFile' in d and 'PulseRate' in d]\n",
        "        valid_data = sorted(valid_data, key=lambda x: x['ImageFile'])\n",
        "\n",
        "        # Build all images & labels\n",
        "        images = []\n",
        "        labels = []\n",
        "        for entry in valid_data:\n",
        "            img_file = entry['ImageFile']\n",
        "            label = entry['PulseRate']\n",
        "            full_img_path = os.path.join(folder_path, img_file)\n",
        "            if os.path.exists(full_img_path):\n",
        "                images.append(full_img_path)\n",
        "                labels.append(label)\n",
        "\n",
        "        # Create sliding windows\n",
        "        num_sequences = len(images) - sequence_length + 1\n",
        "        if num_sequences <= 0:\n",
        "            continue\n",
        "\n",
        "        for i in range(num_sequences):\n",
        "            seq_image_paths = images[i : i + sequence_length]\n",
        "            seq_label = labels[i + sequence_length - 1]\n",
        "            pairs.append((seq_image_paths, seq_label))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "# 5. Generator function to yield pairs (needed for from_generator)\n",
        "def pairs_generator(pairs_list):\n",
        "    for seq_image_paths, label in pairs_list:\n",
        "        yield (seq_image_paths, label)\n",
        "\n",
        "# 6. parse_sequence function using tf.io & tf.image\n",
        "def parse_sequence(seq_image_paths, seq_label):\n",
        "    \"\"\"\n",
        "    seq_image_paths: string tensor of shape (sequence_length,)\n",
        "    seq_label: scalar float\n",
        "    Returns: (images, label)\n",
        "        - images: float32 tensor [sequence_length, H, W, C]\n",
        "        - label: float32 scalar\n",
        "    \"\"\"\n",
        "    def _load_image(path):\n",
        "        # Read file & decode\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_jpeg(img, channels=IMAGE_CHANNELS)\n",
        "        # Resize\n",
        "        #img = tf.image.resize(img, [IMAGE_HEIGHT, IMAGE_WIDTH])\n",
        "        # Normalize\n",
        "        img = tf.cast(img, tf.float32) #process the input for ResNet, from unit8 to float32\n",
        "        img = keras.applications.resnet.preprocess_input(img)\n",
        "        return img\n",
        "\n",
        "    # Apply _load_image to each path in seq_image_paths\n",
        "    images = tf.map_fn(_load_image, seq_image_paths, fn_output_signature=tf.float32)\n",
        "    label = tf.cast(seq_label, tf.float32)\n",
        "    return images, label\n",
        "\n",
        "# 7. Build the final tf.data Datasets\n",
        "def make_dataset(pairs_list, sequence_length, batch_size, shuffle_buffer_size, seed=42):\n",
        "    \"\"\"\n",
        "    Build a tf.data.Dataset using from_generator -> parse_sequence -> batch\n",
        "    \"\"\"\n",
        "    output_signature = (\n",
        "        tf.TensorSpec(shape=(sequence_length,), dtype=tf.string),  # image paths\n",
        "        tf.TensorSpec(shape=(), dtype=tf.float32),                 # label\n",
        "    )\n",
        "\n",
        "    ds = tf.data.Dataset.from_generator(\n",
        "        lambda: pairs_generator(pairs_list),\n",
        "        output_signature=output_signature\n",
        "    )\n",
        "\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size, seed=seed)\n",
        "    ds = ds.map(parse_sequence, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# 8. Create train/val/test pairs + datasets\n",
        "train_pairs = get_image_label_pairs(train_folders, SEQUENCE_LENGTH, DATA_DIR)\n",
        "val_pairs = get_image_label_pairs(val_folders, SEQUENCE_LENGTH, DATA_DIR)\n",
        "test_pairs = get_image_label_pairs(test_folders, SEQUENCE_LENGTH, DATA_DIR)\n",
        "\n",
        "print(f\"Number of train sequences: {len(train_pairs)}\")\n",
        "print(f\"Number of val sequences:   {len(val_pairs)}\")\n",
        "print(f\"Number of test sequences:  {len(test_pairs)}\")\n",
        "\n",
        "train_dataset = make_dataset(train_pairs, SEQUENCE_LENGTH, BATCH_SIZE, SHUFFLE_BUFFER_SIZE, seed=SEED)\n",
        "val_dataset   = make_dataset(val_pairs, SEQUENCE_LENGTH, BATCH_SIZE, SHUFFLE_BUFFER_SIZE, seed=SEED)\n",
        "test_dataset  = make_dataset(test_pairs, SEQUENCE_LENGTH, BATCH_SIZE, SHUFFLE_BUFFER_SIZE, seed=SEED)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(train_pairs) / BATCH_SIZE)\n",
        "validation_steps = math.ceil(len(val_pairs) / BATCH_SIZE)\n",
        "train_dataset = train_dataset.repeat(steps_per_epoch) \n",
        "\n",
        "# Optional: verify shapes\n",
        "print(\"Verifying Training Dataset:\")\n",
        "for batch_x, batch_y in train_dataset.take(1):\n",
        "    print(\"  Train Batch X:\", batch_x.shape)  # (batch_size, SEQUENCE_LENGTH, H, W, C)\n",
        "    print(\"  Train Batch Y:\", batch_y.shape)  # (batch_size,)\n",
        "\n",
        "print(\"Verifying Validation Dataset:\")\n",
        "for batch_x, batch_y in val_dataset.take(1):\n",
        "    print(\"  Val Batch X:\", batch_x.shape)  # (batch_size, SEQUENCE_LENGTH, H, W, C)\n",
        "    print(\"  Val Batch Y:\", batch_y.shape)  # (batch_size,)\n",
        "\n",
        "print(\"Verifying Testing Dataset:\")\n",
        "for batch_x, batch_y in test_dataset.take(1):\n",
        "    print(\"  Test Batch X:\", batch_x.shape)  # (batch_size, SEQUENCE_LENGTH, H, W, C)\n",
        "    print(\"  Test Batch Y:\", batch_y.shape)  # (batch_size,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "IGX35p1i16d0",
        "outputId": "dec97014-54a4-4fd4-de27-d6e576a4051e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"HR_Estimation_Model\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"HR_Estimation_Model\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ video_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ cast_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ video_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed_e… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">8,589,184</span> │ cast_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)         │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convLSTM1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,507,008</span> │ time_distributed… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ convLSTM1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convLSTM2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ convLSTM2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │ reshape_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Multiply</span>)          │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multiply_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ hr_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ video_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ cast_3 (\u001b[38;5;33mCast\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m224\u001b[0m,   │          \u001b[38;5;34m0\u001b[0m │ video_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ time_distributed_e… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │  \u001b[38;5;34m8,589,184\u001b[0m │ cast_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │ \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m1024\u001b[0m)         │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convLSTM1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │  \u001b[38;5;34m2,507,008\u001b[0m │ time_distributed… │\n",
              "│ (\u001b[38;5;33mConvLSTM2D\u001b[0m)        │ \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ convLSTM1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)           │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ convLSTM2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │    \u001b[38;5;34m295,168\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mConvLSTM2D\u001b[0m)        │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ convLSTM2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ reshape_1 (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m4\u001b[0m)   │        \u001b[38;5;34m260\u001b[0m │ reshape_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │        \u001b[38;5;34m320\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ multiply_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMultiply\u001b[0m)          │ \u001b[38;5;34m64\u001b[0m)               │            │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ multiply_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m8,320\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ hr_output (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,409,093</span> (43.52 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,409,093\u001b[0m (43.52 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,819,653</span> (10.76 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,819,653\u001b[0m (10.76 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,589,440</span> (32.77 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m8,589,440\u001b[0m (32.77 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, models\n",
        "\n",
        "\n",
        "def spatial_encoder_block(input_shape, custom_weights_path='/home/giannisstavrakis/Downloads/vggface2_Keras/vggface2_Keras/model/resnet50_softmax_dim512/weights.h5'):\n",
        "    # Load ResNet50 without top layers; weights not loaded by default.\n",
        "    base_model = tf.keras.applications.ResNet50(include_top=False, input_shape=input_shape, weights=None)\n",
        "\n",
        "    # If custom weights are provided, load them with by_name=True to resolve layer mismatches.\n",
        "    if custom_weights_path is not None:\n",
        "        base_model.load_weights(custom_weights_path, by_name=True)\n",
        "\n",
        "    # Optionally, freeze the base model to preserve pretrained features.\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Extract an intermediate feature map that preserves spatial dimensions.\n",
        "    # For a 224x224 input, \"conv4_block6_out\" typically gives a 14x14 feature map.\n",
        "    feature_layer_name = \"conv4_block6_out\"\n",
        "    feature_output = base_model.get_layer(feature_layer_name).output\n",
        "\n",
        "    encoder_model = models.Model(inputs=base_model.input, outputs=feature_output, name=\"spatial_encoder\")\n",
        "    return encoder_model\n",
        "\n",
        "def se_block(inputs, ratio=16):\n",
        "  \n",
        "    channel = inputs.shape[-1]\n",
        "    se = layers.GlobalAveragePooling2D()(inputs)\n",
        "    se = layers.Reshape((1, 1, channel))(se)\n",
        "    se = layers.Dense(channel // ratio, activation='relu', kernel_initializer='he_normal')(se)\n",
        "    se = layers.Dense(channel, activation='sigmoid', kernel_initializer='he_normal')(se)\n",
        "    x = layers.multiply([inputs, se])\n",
        "    return x\n",
        "\n",
        "def build_hr_estimation_model(SEQUENCE_LENGTH, frame_shape):\n",
        "   \n",
        "    video_input = layers.Input(shape=(SEQUENCE_LENGTH, *frame_shape), name=\"video_input\")\n",
        "\n",
        "    # Apply the spatial encoder to each frame via TimeDistributed\n",
        "    spatial_encoder = spatial_encoder_block(frame_shape)\n",
        "    encoded_frames = layers.TimeDistributed(spatial_encoder, name=\"time_distributed_encoder\")(video_input)\n",
        "\n",
        "    # Stacked ConvLSTM layers for spatiotemporal feature extraction\n",
        "    x = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                          return_sequences=True, activation='tanh', name=\"convLSTM1\")(encoded_frames)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), padding='same',\n",
        "                          return_sequences=False, activation='tanh', name=\"convLSTM2\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Apply an attention mechanism (SE block)\n",
        "    x = se_block(x)\n",
        "\n",
        "    # Global pooling and fully connected layers for regression\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(128, activation='relu')(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    hr_output = layers.Dense(1, activation='linear', name=\"hr_output\")(x)\n",
        "\n",
        "    model = models.Model(inputs=video_input, outputs=hr_output, name=\"HR_Estimation_Model\")\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Define hyperparameters\n",
        "    SEQUENCE_LENGTH = SEQUENCE_LENGTH   # Number of frames in each input sequence\n",
        "    frame_shape = (224, 224, 3)  # e.g., 64x64 RGB images; adjust based on your dataset\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = build_hr_estimation_model(SEQUENCE_LENGTH, frame_shape)\n",
        "    base_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_optimizer, dynamic=True)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mae'])\n",
        "\n",
        "    # Print model summary to verify the architecture\n",
        "    model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, max_lr, total_steps, start_lr=None, end_lr=None, verbose=0):\n",
        "        super(OneCycleScheduler, self).__init__()\n",
        "        self.max_lr = max_lr\n",
        "        self.total_steps = total_steps\n",
        "        self.start_lr = start_lr if start_lr is not None else max_lr / 10.0\n",
        "        self.end_lr = end_lr if end_lr is not None else self.start_lr / 100.0\n",
        "        self.verbose = verbose\n",
        "        self.iterations = 0\n",
        "\n",
        "    def set_optimizer_lr(self, value):\n",
        "        optimizer = self.model.optimizer\n",
        "        # For LossScaleOptimizer, update the inner optimizer's learning rate.\n",
        "        if hasattr(optimizer, '_optimizer'):\n",
        "            target_lr = optimizer._optimizer.learning_rate\n",
        "        else:\n",
        "            target_lr = optimizer.learning_rate\n",
        "        # Directly assign the new learning rate.\n",
        "        target_lr.assign(value)\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.set_optimizer_lr(self.start_lr)\n",
        "        if self.verbose > 0:\n",
        "            print(f\"Starting learning rate: {self.start_lr:.6f}\")\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.iterations += 1\n",
        "        lr = self.compute_lr()\n",
        "        self.set_optimizer_lr(lr)\n",
        "        if self.verbose > 0:\n",
        "            print(f\"\\nIteration {self.iterations}: Learning rate is {lr:.6f}\")\n",
        "\n",
        "    def compute_lr(self):\n",
        "        progress = self.iterations / self.total_steps\n",
        "        if progress < 0.5:\n",
        "            # Linear warm-up from start_lr to max_lr.\n",
        "            lr = self.start_lr + (self.max_lr - self.start_lr) * (progress / 0.5)\n",
        "        else:\n",
        "            # Cosine decay from max_lr to end_lr.\n",
        "            cosine_progress = (progress - 0.5) / 0.5\n",
        "            lr = self.end_lr + (self.max_lr - self.end_lr) * 0.5 * (1 + np.cos(np.pi * cosine_progress))\n",
        "        return lr\n",
        "\n",
        "\n",
        "\n",
        "# Variables\n",
        "epochs = 40\n",
        "total_steps = steps_per_epoch * epochs\n",
        "\n",
        "# Define your learning rate parameters\n",
        "start_lr = 0.0001   # starting learning rate\n",
        "max_lr = 0.001     # maximum learning rate during warm-up\n",
        "end_lr = 0.00001    # final learning rate at the end of training\n",
        "\n",
        "one_cycle = OneCycleScheduler(max_lr=max_lr, total_steps=total_steps, start_lr=start_lr, end_lr=end_lr, verbose=1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KXes_0NPd0_U"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# 8. Define Callbacks\n",
        "# ------------------------------------------------------------------------------\n",
        "checkpoint_dir = 'model_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, mode='min')\n",
        "\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=os.path.join(checkpoint_dir, '03_04/best_cnn_lstm_model.weights.h5'),\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    verbose=1,\n",
        "    min_lr=1e-7,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='logs', histogram_freq=1, write_graph=True, write_images=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o9KHRzbd42w",
        "outputId": "e992fffd-425a-4745-ab22-7b3d95d4c131"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 10. Train the Model\n",
        "# ------------------------------------------------------------------------------\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[early_stopping, model_checkpoint,tensorboard,one_cycle]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrKsH0kfd-aP"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "# 11. Plot the Results\n",
        "# ------------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Val MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVOZCUba9WNw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "base_path = \"weights\"\n",
        "\n",
        "# Generate a folder name using the current date and time\n",
        "folder_name = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
        "# Optionally, add a prefix or suffix\n",
        "folder_name = f\"folder_{folder_name}\"\n",
        "\n",
        "# Create the full path by joining the base path with the folder name\n",
        "full_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the new folder; exist_ok=True avoids errors if the folder already exists\n",
        "os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "source_path = \"model_checkpoints/03_04/best_cnn_lstm_model.weights.h5\"\n",
        "\n",
        "shutil.copy(source_path, full_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oWw0Lm4QLxWc"
      },
      "outputs": [],
      "source": [
        "model.save(\"03_04/model.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OUo1XNgL1-H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "base_path = \"weights\"\n",
        "\n",
        "# Generate a folder name using the current date and time\n",
        "folder_name = datetime.now().strftime(\"%d%m%Y_%H%M\")\n",
        "# Optionally, add a prefix or suffix\n",
        "folder_name = f\"folder_{folder_name}\"\n",
        "\n",
        "# Create the full path by joining the base path with the folder name\n",
        "full_path = os.path.join(base_path, folder_name)\n",
        "\n",
        "# Create the new folder; exist_ok=True avoids errors if the folder already exists\n",
        "os.makedirs(full_path, exist_ok=True)\n",
        "\n",
        "source_path = \"03_04/model.keras\"\n",
        "\n",
        "shutil.copy(source_path, full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77cwHklZMFCa"
      },
      "outputs": [],
      "source": [
        "model.save_weights('/content/drive/MyDrive/5thYear/weights/folder_14022025_1936/model_weights.weights.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbUunp_dIdDM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. (Optional) Load the best weights from your checkpoint\n",
        "best_weights_path = \"model_checkpoints/03_04/best_cnn_lstm_model.weights.h5\"\n",
        "model.load_weights(best_weights_path)\n",
        "\n",
        "# 2. Evaluate on the test dataset\n",
        "    #This uses the same metrics you specified in model.compile(...)\n",
        "test_loss, test_mae = model.evaluate(test_dataset)\n",
        "print(f\"Test Loss (e.g. MSE): {test_loss:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "# 3. (Optional) Get actual predictions and compare to ground truth\n",
        "#    - We'll collect predictions for every batch\n",
        "predictions = model.predict(test_dataset)  # shape (total_samples, 1) for Dense(1)\n",
        "\n",
        "#    - Also collect labels from the test dataset\n",
        "all_labels = []\n",
        "for _, y in test_dataset:\n",
        "    all_labels.append(y.numpy())\n",
        "\n",
        "all_labels = np.concatenate(all_labels, axis=0)   # shape (total_samples,)\n",
        "predictions = predictions.squeeze(axis=-1)        # shape (total_samples,)\n",
        "\n",
        "# 4. Compute a manual MSE or any other metric you like\n",
        "manual_mse = np.mean((predictions - all_labels)**2)\n",
        "print(f\"Manual MSE on Test Set: {manual_mse:.4f}\")\n",
        "\n",
        "# 5. (Optional) Inspect a few predictions vs. ground truths\n",
        "for i in range(5):\n",
        "    print(f\"Sample {i}: Prediction={predictions[i]:.3f}, Ground Truth={all_labels[i]:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
